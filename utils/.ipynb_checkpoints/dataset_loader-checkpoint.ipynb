{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateDataset:\n",
    "    def __init__(self, device, use_stopword=True):\n",
    "        \n",
    "        ### cpu or gpu setting ###\n",
    "        self.device = device\n",
    "        \n",
    "        ### stopword 사용 여부 변수 ###\n",
    "        self.use_stopword = use_stopword\n",
    "        \n",
    "        ### spacy library를 활용하여 tokenizer에 활용 ###\n",
    "        self.spacy_de = spacy.load('de_core_news_sm')\n",
    "        self.spacy_en = spacy.load('en_core_web_sm')\n",
    "        \n",
    "        ### spacy에서 제공하는 stop word list ###\n",
    "        self.de_stopwords = self.spacy_de.Defaults.stop_words\n",
    "        self.en_stopwords = self.spacy_en.Defaults.stop_words\n",
    "        \n",
    "        ### 데이터 셋의 전처리에 관한 부분을 담고 있음 ###\n",
    "        ### tokenize는 아래의 함수를 활용하며, 시작 토큰에 <sos>를 추가, 끝 토큰엔 <eos>를 추가하고\n",
    "        ### 소문자로 정규화\n",
    "        self.SRC = Field(tokenize=self.tokenize_de, init_token='<sos>', eos_token='<eos>', lower=True)\n",
    "        self.TRG = Field(tokenize=self.tokenize_en, init_token='<sos>', eos_token='<eos>', lower=True)\n",
    "        \n",
    "        ### torchtext.data로 부터 de (독일어), en(영어) 데이터 셋을 가져와서\n",
    "        ### 앞전에 선언해 둔 전처리 함수를 넣어줘서 전처리된 값을 return 받는다\n",
    "        self.train_data, self.valid_data, self.test_data = Multi30k.splits(exts=('.de', '.en'), fields=(self.SRC, self.TRG))\n",
    "        \n",
    "        ### vocabulary를 만드는 과정으로 train_data에 있는 단어들을 가지고\n",
    "        ### 2번 이상 등장한 단어들로 vocab을 만든다.\n",
    "        self.SRC.build_vocab(self.train_data, min_freq = 2)\n",
    "        self.TRG.build_vocab(self.train_data, min_freq = 2)\n",
    "    \n",
    "    ### 전처리에 들어갈 tokenize 함수들\n",
    "    def tokenize_de(self, text)\n",
    "        if self.use_stopword:\n",
    "            return [tok.text for tok in self.spacy_de.tokenizer(text)]\n",
    "        return [tok.text for tok in self.spacy_de.tokenizer(text) if tok not in self.de_stopwords]\n",
    "    def tokenize_en(self, text):\n",
    "        if self.use_stopword:\n",
    "            return [tok.text for tok in self.spacy_en.tokenizer(text)]\n",
    "        return [tok.text for tok in self.spacy_de.tokenizer(text) if tok not in self.en_stopwords]\n",
    "\n",
    "    ### 최종적으로 iterator에 데이터 셋을 담아서 iterator를 반환\n",
    "    ### 함수에선 해당 iterator로 부터 데이터를 뽑아내어 사용할 것\n",
    "    def get_iterator(self, batch_size):\n",
    "        train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "            (self.train_data, self.valid_data, self.test_data), \n",
    "            batch_size = batch_size, \n",
    "            device = self.device)\n",
    "        return train_iterator, valid_iterator, test_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "#     device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#     dataset = CreateDataset(device, use_stopword=False)\n",
    "#     train, valid, test = dataset.get_iterator(2)\n",
    "#     for x in train:\n",
    "#         print(x.src)\n",
    "#         print(x.trg)\n",
    "#         break\n",
    "#     for x in valid:\n",
    "#         print(x.src)\n",
    "#         print(x.trg)\n",
    "#         break\n",
    "#     for x in test:\n",
    "#         print(x.src)\n",
    "#         print(x.trg)\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
